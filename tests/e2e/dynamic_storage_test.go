/*
Copyright Â© contributors to CloudNativePG, established as
CloudNativePG a Series of LF Projects, LLC.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.

SPDX-License-Identifier: Apache-2.0
*/

package e2e

import (
	"fmt"
	"strconv"
	"strings"
	"time"

	corev1 "k8s.io/api/core/v1"
	"k8s.io/apimachinery/pkg/api/resource"
	"k8s.io/apimachinery/pkg/types"
	"k8s.io/utils/ptr"
	ctrlclient "sigs.k8s.io/controller-runtime/pkg/client"

	apiv1 "github.com/cloudnative-pg/cloudnative-pg/api/v1"
	"github.com/cloudnative-pg/cloudnative-pg/pkg/reconciler/dynamicstorage"
	"github.com/cloudnative-pg/cloudnative-pg/pkg/specs"
	"github.com/cloudnative-pg/cloudnative-pg/pkg/utils"
	"github.com/cloudnative-pg/cloudnative-pg/tests"
	"github.com/cloudnative-pg/cloudnative-pg/tests/utils/clusterutils"
	"github.com/cloudnative-pg/cloudnative-pg/tests/utils/exec"
	"github.com/cloudnative-pg/cloudnative-pg/tests/utils/nodes"
	"github.com/cloudnative-pg/cloudnative-pg/tests/utils/operator"
	podutils "github.com/cloudnative-pg/cloudnative-pg/tests/utils/pods"
	"github.com/cloudnative-pg/cloudnative-pg/tests/utils/postgres"
	"github.com/cloudnative-pg/cloudnative-pg/tests/utils/proxy"
	"github.com/cloudnative-pg/cloudnative-pg/tests/utils/timeouts"

	. "github.com/onsi/ginkgo/v2"
	. "github.com/onsi/gomega"
)

// getDiskUsagePercent returns the disk usage percentage for the PGDATA directory
// by executing `df` inside the postgres container and parsing the output.
func getDiskUsagePercent(
	pod *corev1.Pod,
) (int, error) {
	timeout := 10 * time.Second
	// Use df to get usage of /var/lib/postgresql/data (PGDATA mount)
	stdout, stderr, err := exec.CommandInContainer(
		env.Ctx, env.Client, env.Interface, env.RestClientConfig,
		exec.ContainerLocator{
			Namespace:     pod.Namespace,
			PodName:       pod.Name,
			ContainerName: specs.PostgresContainerName,
		},
		&timeout,
		"df", "--output=pcent", "/var/lib/postgresql/data",
	)
	if err != nil {
		return 0, fmt.Errorf("df command failed: %w, stderr: %s", err, stderr)
	}

	// Parse output - format is:
	// Use%
	//  42%
	lines := strings.Split(strings.TrimSpace(stdout), "\n")
	if len(lines) < 2 {
		return 0, fmt.Errorf("unexpected df output: %s", stdout)
	}
	usageStr := strings.TrimSpace(lines[1])
	usageStr = strings.TrimSuffix(usageStr, "%")
	usage, err := strconv.Atoi(usageStr)
	if err != nil {
		return 0, fmt.Errorf("failed to parse usage percentage from '%s': %w", usageStr, err)
	}
	return usage, nil
}

// getTablespaceDiskUsagePercent returns the disk usage percentage for a specific tablespace
func getTablespaceDiskUsagePercent(
	pod *corev1.Pod,
	tbsName string,
) (int, error) {
	timeout := 10 * time.Second
	tsPath := specs.MountForTablespace(tbsName)
	stdout, stderr, err := exec.CommandInContainer(
		env.Ctx, env.Client, env.Interface, env.RestClientConfig,
		exec.ContainerLocator{
			Namespace:     pod.Namespace,
			PodName:       pod.Name,
			ContainerName: specs.PostgresContainerName,
		},
		&timeout,
		"df", "--output=pcent", tsPath,
	)
	if err != nil {
		return 0, fmt.Errorf("df command failed for tablespace %s: %w, stderr: %s", tbsName, err, stderr)
	}

	lines := strings.Split(strings.TrimSpace(stdout), "\n")
	if len(lines) < 2 {
		return 0, fmt.Errorf("unexpected df output for tablespace %s: %s", tbsName, stdout)
	}
	usageStr := strings.TrimSpace(lines[1])
	usageStr = strings.TrimSuffix(usageStr, "%")
	usage, err := strconv.Atoi(usageStr)
	if err != nil {
		return 0, fmt.Errorf("failed to parse usage percentage for tablespace %s from '%s': %w",
			tbsName, usageStr, err)
	}
	return usage, nil
}

// fillTablespaceDiskIncrementally fills the disk on a specific tablespace
func fillTablespaceDiskIncrementally(
	pod *corev1.Pod,
	tbsName string,
	targetUsagePercent int,
	maxUsagePercent int,
	batchRows int,
) (int, error) {
	GinkgoWriter.Printf("Starting incremental disk fill on tablespace %s, pod %s, target: %d%%, max: %d%%\n",
		tbsName, pod.Name, targetUsagePercent, maxUsagePercent)

	currentUsage, err := getTablespaceDiskUsagePercent(pod, tbsName)
	if err != nil {
		return 0, fmt.Errorf("failed to get initial tablespace disk usage: %w", err)
	}

	// Create the fill table in the tablespace
	createTableQuery := fmt.Sprintf("CREATE TABLE IF NOT EXISTS fill_tbs (id bigint, data text) TABLESPACE %s;",
		tbsName)
	timeout := time.Minute
	_, _, err = exec.QueryInInstancePodWithTimeout(
		env.Ctx, env.Client, env.Interface, env.RestClientConfig,
		exec.PodLocator{Namespace: pod.Namespace, PodName: pod.Name},
		postgres.AppDBName,
		createTableQuery,
		timeout,
	)
	if err != nil {
		return currentUsage, fmt.Errorf("failed to create fill table in tablespace: %w", err)
	}

	batchNum := 0
	for currentUsage < targetUsagePercent {
		batchNum++
		if currentUsage >= maxUsagePercent {
			break
		}

		startID := (batchNum-1)*batchRows + 1
		endID := batchNum * batchRows
		insertQuery := fmt.Sprintf(
			"INSERT INTO fill_tbs SELECT id, repeat('x', 1000) FROM generate_series(%d, %d) AS id;",
			startID, endID,
		)

		_, _, err := exec.QueryInInstancePodWithTimeout(
			env.Ctx, env.Client, env.Interface, env.RestClientConfig,
			exec.PodLocator{Namespace: pod.Namespace, PodName: pod.Name},
			postgres.AppDBName,
			insertQuery,
			time.Minute*5,
		)
		if err != nil {
			return currentUsage, err
		}

		time.Sleep(2 * time.Second)
		currentUsage, _ = getTablespaceDiskUsagePercent(pod, tbsName)
	}

	return currentUsage, nil
}

// fillDiskIncrementally fills the disk on the given pod incrementally until
// the target usage percentage is reached. It inserts data in batches and checks
// disk usage between batches to avoid overshooting.
// Parameters:
//   - pod: the pod to fill disk on
//   - targetUsagePercent: stop when this usage is reached (e.g., 85 for 85%)
//   - maxUsagePercent: abort if usage exceeds this to prevent crash (e.g., 95)
//   - batchRows: number of rows per batch (e.g., 500000)
//
// Returns the final disk usage percentage or error.
func fillDiskIncrementally(
	pod *corev1.Pod,
	targetUsagePercent int,
	maxUsagePercent int,
	batchRows int,
) (int, error) {
	GinkgoWriter.Printf("Starting incremental disk fill on pod %s, target: %d%%, max: %d%%\n",
		pod.Name, targetUsagePercent, maxUsagePercent)

	// First, check current usage
	currentUsage, err := getDiskUsagePercent(pod)
	if err != nil {
		return 0, fmt.Errorf("failed to get initial disk usage: %w", err)
	}
	GinkgoWriter.Printf("Initial disk usage: %d%%\n", currentUsage)

	if currentUsage >= targetUsagePercent {
		GinkgoWriter.Printf("Disk already at target usage (%d%% >= %d%%)\n", currentUsage, targetUsagePercent)
		return currentUsage, nil
	}

	// Create the fill table if it doesn't exist
	createTableQuery := "CREATE TABLE IF NOT EXISTS fill_data (id bigint, data text);"
	timeout := time.Minute
	_, _, err = exec.QueryInInstancePodWithTimeout(
		env.Ctx, env.Client, env.Interface, env.RestClientConfig,
		exec.PodLocator{Namespace: pod.Namespace, PodName: pod.Name},
		postgres.AppDBName,
		createTableQuery,
		timeout,
	)
	if err != nil {
		return currentUsage, fmt.Errorf("failed to create fill table: %w", err)
	}

	batchNum := 0
	for currentUsage < targetUsagePercent {
		batchNum++

		// Safety check - abort if we're getting too close to full
		if currentUsage >= maxUsagePercent {
			GinkgoWriter.Printf("Stopping: usage %d%% exceeds max %d%%\n", currentUsage, maxUsagePercent)
			break
		}

		// Insert a batch of data
		// Each row is approximately 1KB (id + 1000 chars)
		startID := (batchNum-1)*batchRows + 1
		endID := batchNum * batchRows
		insertQuery := fmt.Sprintf(
			"INSERT INTO fill_data SELECT id, repeat('x', 1000) FROM generate_series(%d, %d) AS id;",
			startID, endID,
		)

		GinkgoWriter.Printf("Batch %d: inserting rows %d-%d (current usage: %d%%)\n",
			batchNum, startID, endID, currentUsage)

		_, stderr, err := exec.QueryInInstancePodWithTimeout(
			env.Ctx, env.Client, env.Interface, env.RestClientConfig,
			exec.PodLocator{Namespace: pod.Namespace, PodName: pod.Name},
			postgres.AppDBName,
			insertQuery,
			time.Minute*5,
		)
		if err != nil {
			// Check if it's a disk full error
			if strings.Contains(stderr, "No space left on device") {
				GinkgoWriter.Printf("Disk full error during batch %d\n", batchNum)
				return currentUsage, fmt.Errorf("disk full during batch %d: %w", batchNum, err)
			}
			return currentUsage, fmt.Errorf("insert failed during batch %d: %w, stderr: %s", batchNum, err, stderr)
		}

		// Wait a bit for WAL to flush and filesystem to update
		time.Sleep(2 * time.Second)

		// Check disk usage after batch
		currentUsage, err = getDiskUsagePercent(pod)
		if err != nil {
			GinkgoWriter.Printf("Warning: failed to get disk usage after batch %d: %v\n", batchNum, err)
			// Continue anyway, we'll check again next iteration
			continue
		}
		GinkgoWriter.Printf("After batch %d: disk usage is %d%%\n", batchNum, currentUsage)
	}

	GinkgoWriter.Printf("Disk fill complete. Final usage: %d%%\n", currentUsage)
	return currentUsage, nil
}

// updateCluster updates a cluster using a mutator function
func updateCluster(namespace, clusterName string, mutator func(*apiv1.Cluster)) {
	cluster, err := clusterutils.Get(env.Ctx, env.Client, namespace, clusterName)
	Expect(err).ToNot(HaveOccurred())
	original := cluster.DeepCopy()
	mutator(cluster)
	err = env.Client.Patch(env.Ctx, cluster, ctrlclient.MergeFrom(original))
	Expect(err).ToNot(HaveOccurred())
}

// verifyGrowthCompletion waits for a growth operation to complete, including:
// 1. StorageSizing status reaching a stable state (Balanced or empty)
// 2. PVC capacity actually reflecting the resize (CSI driver completion)
func verifyGrowthCompletion(namespace, clusterName string) {
	By("waiting for storage sizing state to stabilize", func() {
		Eventually(func(g Gomega) {
			cluster, err := clusterutils.Get(env.Ctx, env.Client, namespace, clusterName)
			g.Expect(err).ToNot(HaveOccurred())
			if cluster.Status.StorageSizing != nil && cluster.Status.StorageSizing.Data != nil {
				state := cluster.Status.StorageSizing.Data.State
				g.Expect(state).To(Or(Equal("Balanced"), Equal("")),
					"Waiting for growth to complete, current state: %s", state)
			}
		}).WithTimeout(time.Duration(testTimeouts[timeouts.AKSVolumeResize]) * time.Second).
			WithPolling(time.Duration(testTimeouts[timeouts.StorageSizingPolling]) * time.Second).Should(Succeed())
	})

	By("waiting for PVC capacity to update (CSI resize completion)", func() {
		waitForPVCCapacityUpdate(namespace, clusterName,
			time.Duration(testTimeouts[timeouts.AKSVolumeResize])*time.Second)
	})
}

// assertDataConsistency verifies that data is consistent across replicas
func assertDataConsistency(namespace, clusterName string) {
	cluster, err := clusterutils.Get(env.Ctx, env.Client, namespace, clusterName)
	Expect(err).ToNot(HaveOccurred())
	Expect(cluster.Status.Phase).To(Equal(apiv1.PhaseHealthy))
}

// waitForPVCCapacityUpdate waits for PVC Status.Capacity to reflect the resize completion.
// This is critical for Azure AKS CSI operations which can take 5-10 minutes.
// The function verifies that all PVCs have capacity >= their request, indicating the
// CSI driver has completed the filesystem resize.
func waitForPVCCapacityUpdate(namespace, clusterName string, timeout time.Duration) {
	GinkgoWriter.Printf("Waiting for PVC capacity update in cluster %s/%s (timeout: %v)\n",
		namespace, clusterName, timeout)

	Eventually(func(g Gomega) {
		var pvcList corev1.PersistentVolumeClaimList
		err := env.Client.List(env.Ctx, &pvcList,
			ctrlclient.InNamespace(namespace),
			ctrlclient.MatchingLabels{utils.ClusterLabelName: clusterName})
		g.Expect(err).ToNot(HaveOccurred())
		g.Expect(pvcList.Items).ToNot(BeEmpty(), "Expected at least one PVC for cluster")

		for _, pvc := range pvcList.Items {
			request := pvc.Spec.Resources.Requests[corev1.ResourceStorage]
			capacity := pvc.Status.Capacity[corev1.ResourceStorage]

			// Capacity should be >= request once resize is complete
			g.Expect(capacity.Cmp(request)).To(BeNumerically(">=", 0),
				"PVC %s: capacity (%s) should be >= request (%s)",
				pvc.Name, capacity.String(), request.String())

			GinkgoWriter.Printf("PVC %s: request=%s, capacity=%s (OK)\n",
				pvc.Name, request.String(), capacity.String())
		}
	}).WithTimeout(timeout).
		WithPolling(time.Duration(testTimeouts[timeouts.StorageSizingPolling]) * time.Second).Should(Succeed())

	GinkgoWriter.Printf("All PVCs have updated capacity\n")
}

var _ = Describe("Dynamic Storage", Label(tests.LabelStorage, tests.LabelDynamicStorage), func() {
	const (
		level           = tests.Medium
		namespacePrefix = "dynamic-storage-e2e"
	)

	var namespace string

	BeforeEach(func() {
		if testLevelEnv.Depth < int(level) {
			Skip("Test depth is lower than the amount requested for this test")
		}
		if MustGetEnvProfile().UsesNodeDiskSpace() {
			Skip("this test requires dynamic volume provisioning with resize support")
		}
	})

	Context("Tablespace dynamic sizing", Label(tests.LabelDynamicStorage), func() {
		It("grows tablespace storage when usage exceeds target buffer", func() {
			var err error
			namespace, err = env.CreateUniqueTestNamespace(env.Ctx, env.Client, namespacePrefix)
			Expect(err).ToNot(HaveOccurred())

			clusterName := "dynamic-tbs-grow"
			tbsName := "tbsdynamic"
			clusterFile := fixturesDir + "/dynamic_storage/cluster-tablespaces-dynamic.yaml.template"

			By("creating cluster with tablespaces", func() {
				AssertCreateCluster(namespace, clusterName, clusterFile, env)
			})

			var primaryPod *corev1.Pod
			By("finding primary pod", func() {
				primaryPod, err = clusterutils.GetPrimary(env.Ctx, env.Client, namespace, clusterName)
				Expect(err).ToNot(HaveOccurred())
			})

			By("filling tablespace disk to trigger growth", func() {
				finalUsage, err := fillTablespaceDiskIncrementally(primaryPod, tbsName, 85, 92, 500000)
				if err != nil {
					GinkgoWriter.Printf("Tablespace disk fill ended with error: %v\n", err)
				}
				Expect(finalUsage).To(BeNumerically(">=", 75))
			})

			By("verifying tablespace storage sizing status is updated", func() {
				Eventually(func(g Gomega) {
					cluster, err := clusterutils.Get(env.Ctx, env.Client, namespace, clusterName)
					g.Expect(err).ToNot(HaveOccurred())
					g.Expect(cluster.Status.StorageSizing).ToNot(BeNil())
					g.Expect(cluster.Status.StorageSizing.Tablespaces).ToNot(BeNil())
					g.Expect(cluster.Status.StorageSizing.Tablespaces[tbsName]).ToNot(BeNil())

					var pvcList corev1.PersistentVolumeClaimList
					err = env.Client.List(env.Ctx, &pvcList,
						ctrlclient.InNamespace(namespace),
						ctrlclient.MatchingLabels{
							utils.ClusterLabelName:        clusterName,
							utils.PvcRoleLabelName:        string(utils.PVCRolePgTablespace),
							utils.TablespaceNameLabelName: tbsName,
						})
					g.Expect(err).ToNot(HaveOccurred())
					g.Expect(pvcList.Items).To(HaveLen(1))
					size := pvcList.Items[0].Spec.Resources.Requests[corev1.ResourceStorage]
					g.Expect(size.Cmp(resource.MustParse("1Gi"))).To(BeNumerically(">", 0),
						"Tablespace PVC request should grow beyond initial 1Gi")
				}).WithTimeout(time.Duration(testTimeouts[timeouts.AKSVolumeResize]) * time.Second).
					WithPolling(time.Duration(testTimeouts[timeouts.AKSPollingInterval]) * time.Second).Should(Succeed())
			})
		})
	})

	Context("Dynamic sizing validation", func() {
		It("rejects invalid configurations", func() {
			var err error
			namespace, err = env.CreateUniqueTestNamespace(env.Ctx, env.Client, namespacePrefix)
			Expect(err).ToNot(HaveOccurred())

			By("rejecting size with request/limit", func() {
				cluster := &apiv1.Cluster{}
				cluster.Name = "invalid-config"
				cluster.Namespace = namespace
				cluster.Spec.Instances = 1
				cluster.Spec.StorageConfiguration = apiv1.StorageConfiguration{
					Size:    "10Gi",
					Request: "5Gi",
					Limit:   "20Gi",
				}

				err := env.Client.Create(env.Ctx, cluster)
				Expect(err).To(HaveOccurred())
				Expect(err.Error()).To(ContainSubstring("mutually exclusive"))
			})

			By("rejecting request without limit", func() {
				cluster := &apiv1.Cluster{}
				cluster.Name = "invalid-config-2"
				cluster.Namespace = namespace
				cluster.Spec.Instances = 1
				cluster.Spec.StorageConfiguration = apiv1.StorageConfiguration{
					Request: "5Gi",
				}

				err := env.Client.Create(env.Ctx, cluster)
				Expect(err).To(HaveOccurred())
				Expect(err.Error()).To(ContainSubstring("limit is required"))
			})

			By("rejecting request greater than limit", func() {
				cluster := &apiv1.Cluster{}
				cluster.Name = "invalid-config-3"
				cluster.Namespace = namespace
				cluster.Spec.Instances = 1
				cluster.Spec.StorageConfiguration = apiv1.StorageConfiguration{
					Request: "20Gi",
					Limit:   "10Gi",
				}

				err := env.Client.Create(env.Ctx, cluster)
				Expect(err).To(HaveOccurred())
				Expect(err.Error()).To(ContainSubstring("cannot exceed limit"))
			})
		})
	})

	Context("Dynamic sizing functionality", Label(tests.LabelDynamicStorage), func() {
		It("provisions PVC at request size", func() {
			var err error
			namespace, err = env.CreateUniqueTestNamespace(env.Ctx, env.Client, namespacePrefix)
			Expect(err).ToNot(HaveOccurred())

			clusterName := "dynamic-basic"
			cluster := &apiv1.Cluster{}
			cluster.Name = clusterName
			cluster.Namespace = namespace
			cluster.Spec.Instances = 1
			cluster.Spec.StorageConfiguration = apiv1.StorageConfiguration{
				Request:      "5Gi",
				Limit:        "20Gi",
				TargetBuffer: ptr.To(20),
			}

			By("creating cluster", func() {
				err := env.Client.Create(env.Ctx, cluster)
				Expect(err).ToNot(HaveOccurred())

				AssertClusterIsReady(namespace, clusterName, testTimeouts[timeouts.ClusterIsReady], env)
			})

			By("verifying PVC is created at request size", func() {
				var pvcList corev1.PersistentVolumeClaimList
				Eventually(func(g Gomega) {
					err := env.Client.List(env.Ctx, &pvcList,
						ctrlclient.InNamespace(namespace),
						ctrlclient.MatchingLabels{utils.ClusterLabelName: clusterName})
					g.Expect(err).ToNot(HaveOccurred())
					g.Expect(pvcList.Items).To(HaveLen(1))
					size := pvcList.Items[0].Spec.Resources.Requests[corev1.ResourceStorage]
					g.Expect(size.String()).To(Equal("5Gi"))
				}).WithTimeout(time.Duration(testTimeouts[timeouts.AKSVolumeResize]) * time.Second).
					WithPolling(time.Duration(testTimeouts[timeouts.AKSPollingInterval]) * time.Second).Should(Succeed())
			})

			By("verifying dynamic sizing is detected as enabled", func() {
				Expect(dynamicstorage.IsDynamicSizingEnabled(&cluster.Spec.StorageConfiguration)).To(BeTrue())
			})

			By("verifying Prometheus metrics are exposed", func() {
				podList, err := clusterutils.ListPods(env.Ctx, env.Client, namespace, clusterName)
				Expect(err).ToNot(HaveOccurred())

				for _, pod := range podList.Items {
					Eventually(func(g Gomega) {
						out, err := proxy.RetrieveMetricsFromInstance(env.Ctx, env.Interface, pod, false)
						g.Expect(err).ToNot(HaveOccurred())

						// Check for disk metrics
						g.Expect(out).To(ContainSubstring("cnpg_disk_total_bytes"))
						g.Expect(out).To(ContainSubstring("cnpg_disk_used_bytes"))

						// Check for dynamic storage metrics
						g.Expect(out).To(ContainSubstring("cnpg_dynamic_storage_target_size_bytes"))
						g.Expect(out).To(ContainSubstring("cnpg_dynamic_storage_state"))
						g.Expect(out).To(ContainSubstring("cnpg_dynamic_storage_budget_total"))
					}, testTimeouts[timeouts.Short]).Should(Succeed())
				}
			})
		})

		It("grows storage when usage exceeds target buffer", func() {
			var err error
			namespace, err = env.CreateUniqueTestNamespace(env.Ctx, env.Client, namespacePrefix)
			Expect(err).ToNot(HaveOccurred())

			clusterName := "dynamic-grow"
			cluster := &apiv1.Cluster{}
			cluster.Name = clusterName
			cluster.Namespace = namespace
			cluster.Spec.Instances = 1
			cluster.Spec.StorageConfiguration = apiv1.StorageConfiguration{
				Request:      "5Gi",
				Limit:        "20Gi",
				TargetBuffer: ptr.To(20),
				// Maintenance window is always open by default
			}

			By("creating cluster", func() {
				err := env.Client.Create(env.Ctx, cluster)
				Expect(err).ToNot(HaveOccurred())

				AssertClusterIsReady(namespace, clusterName, testTimeouts[timeouts.ClusterIsReady], env)
			})

			var primaryPod *corev1.Pod
			By("finding primary pod", func() {
				primaryPod, err = clusterutils.GetPrimary(env.Ctx, env.Client, namespace, clusterName)
				Expect(err).ToNot(HaveOccurred())
			})

			By("filling disk to trigger growth", func() {
				// Fill disk incrementally to reach ~85% usage (exceeding the 80% threshold
				// that triggers growth when targetBuffer is 20%). We use incremental filling
				// to give the storage reconciler time to detect the condition and respond,
				// avoiding a scenario where the disk fills to 100% before resize can occur.
				// - targetUsagePercent=85: stop when we hit 85% (past the 80% trigger point)
				// - maxUsagePercent=92: safety limit to prevent accidental disk full crash
				// - batchRows=500000: ~500MB per batch, allows reconciler check between batches
				finalUsage, err := fillDiskIncrementally(primaryPod, 85, 92, 500000)
				// Error is acceptable if we reached a high enough usage to trigger resize
				if err != nil {
					GinkgoWriter.Printf("Disk fill ended with error (may be expected): %v\n", err)
				}
				GinkgoWriter.Printf("Final disk usage after fill: %d%%\n", finalUsage)
				// We should have reached at least 75% to trigger the resize logic
				Expect(finalUsage).To(BeNumerically(">=", 75),
					"Disk fill should reach at least 75%% to trigger growth")
			})

			By("verifying storage sizing status is updated", func() {
				// Wait for the dynamic storage reconciler to:
				// 1. Collect disk status from instance manager
				// 2. Evaluate sizing needs based on disk usage
				// 3. Update cluster.Status.StorageSizing
				// This can take several reconciliation cycles (30s-5min)
				Eventually(func(g Gomega) {
					cluster, err := clusterutils.Get(env.Ctx, env.Client, namespace, clusterName)
					g.Expect(err).ToNot(HaveOccurred())
					g.Expect(cluster.Status.StorageSizing).ToNot(BeNil(),
						"StorageSizing status should be populated after disk usage triggers sizing logic")
					g.Expect(cluster.Status.StorageSizing.Data).ToNot(BeNil(),
						"Data volume sizing status should be populated")
					var pvcList corev1.PersistentVolumeClaimList
					err = env.Client.List(env.Ctx, &pvcList,
						ctrlclient.InNamespace(namespace),
						ctrlclient.MatchingLabels{utils.ClusterLabelName: clusterName})
					g.Expect(err).ToNot(HaveOccurred())
					g.Expect(pvcList.Items).To(HaveLen(1))
					size := pvcList.Items[0].Spec.Resources.Requests[corev1.ResourceStorage]
					g.Expect(size.Cmp(resource.MustParse("5Gi"))).To(BeNumerically(">", 0),
						"PVC request should grow beyond initial 5Gi after sizing logic runs")
				}).WithTimeout(time.Duration(testTimeouts[timeouts.AKSVolumeResize]) * time.Second).
					WithPolling(time.Duration(testTimeouts[timeouts.AKSPollingInterval]) * time.Second).Should(Succeed())
			})
		})

		It("respects limit and does not grow beyond it", func() {
			var err error
			namespace, err = env.CreateUniqueTestNamespace(env.Ctx, env.Client, namespacePrefix)
			Expect(err).ToNot(HaveOccurred())

			clusterName := "dynamic-limit"
			cluster := &apiv1.Cluster{}
			cluster.Name = clusterName
			cluster.Namespace = namespace
			cluster.Spec.Instances = 1
			cluster.Spec.StorageConfiguration = apiv1.StorageConfiguration{
				Request:      "5Gi",
				Limit:        "6Gi",
				TargetBuffer: ptr.To(20),
			}

			By("creating cluster", func() {
				err := env.Client.Create(env.Ctx, cluster)
				Expect(err).ToNot(HaveOccurred())

				AssertClusterIsReady(namespace, clusterName, testTimeouts[timeouts.ClusterIsReady], env)
			})

			By("verifying PVC does not exceed limit", func() {
				// Even after growth, PVC should never exceed limit
				Consistently(func(g Gomega) {
					var pvcList corev1.PersistentVolumeClaimList
					err := env.Client.List(env.Ctx, &pvcList,
						ctrlclient.InNamespace(namespace),
						ctrlclient.MatchingLabels{utils.ClusterLabelName: clusterName})
					g.Expect(err).ToNot(HaveOccurred())
					for _, pvc := range pvcList.Items {
						size := pvc.Spec.Resources.Requests[corev1.ResourceStorage]
						limit := resource.MustParse("6Gi")
						g.Expect(size.Cmp(limit)).To(BeNumerically("<=", 0))
					}
				}).WithTimeout(time.Duration(testTimeouts[timeouts.AKSVolumeResize]) * time.Second).
					WithPolling(time.Duration(testTimeouts[timeouts.AKSPollingInterval]) * time.Second).Should(Succeed())
			})
		})

		It("creates new replicas at effective size", func() {
			var err error
			namespace, err = env.CreateUniqueTestNamespace(env.Ctx, env.Client, namespacePrefix)
			Expect(err).ToNot(HaveOccurred())

			clusterName := "dynamic-replica"
			cluster := &apiv1.Cluster{}
			cluster.Name = clusterName
			cluster.Namespace = namespace
			cluster.Spec.Instances = 1
			cluster.Spec.StorageConfiguration = apiv1.StorageConfiguration{
				Request:      "5Gi",
				Limit:        "20Gi",
				TargetBuffer: ptr.To(20),
			}

			By("creating cluster with 1 instance", func() {
				err := env.Client.Create(env.Ctx, cluster)
				Expect(err).ToNot(HaveOccurred())

				AssertClusterIsReady(namespace, clusterName, testTimeouts[timeouts.ClusterIsReady], env)
			})

			By("scaling to 2 instances", func() {
				cluster, err := clusterutils.Get(env.Ctx, env.Client, namespace, clusterName)
				Expect(err).ToNot(HaveOccurred())

				originCluster := cluster.DeepCopy()
				cluster.Spec.Instances = 2
				err = env.Client.Patch(env.Ctx, cluster, ctrlclient.MergeFrom(originCluster))
				Expect(err).ToNot(HaveOccurred())

				AssertClusterIsReady(namespace, clusterName, testTimeouts[timeouts.ClusterIsReady], env)
			})

			By("verifying new replica PVC matches existing size", func() {
				var pvcList corev1.PersistentVolumeClaimList
				Eventually(func(g Gomega) {
					err := env.Client.List(env.Ctx, &pvcList,
						ctrlclient.InNamespace(namespace),
						ctrlclient.MatchingLabels{utils.ClusterLabelName: clusterName})
					g.Expect(err).ToNot(HaveOccurred())
					g.Expect(pvcList.Items).To(HaveLen(2))

					// All PVCs should have the same size
					sizes := make(map[string]bool)
					for _, pvc := range pvcList.Items {
						size := pvc.Spec.Resources.Requests[corev1.ResourceStorage]
						sizes[size.String()] = true
					}
					g.Expect(sizes).To(HaveLen(1))
				}).WithTimeout(time.Duration(testTimeouts[timeouts.AKSVolumeResize]) * time.Second).
					WithPolling(time.Duration(testTimeouts[timeouts.AKSPollingInterval]) * time.Second).Should(Succeed())
			})
		})
	})

	Context("Maintenance window", Label(tests.LabelDynamicStorage), func() {
		It("queues growth when outside maintenance window", func() {
			var err error
			namespace, err = env.CreateUniqueTestNamespace(env.Ctx, env.Client, namespacePrefix)
			Expect(err).ToNot(HaveOccurred())

			clusterName := "dynamic-maintenance"
			cluster := &apiv1.Cluster{}
			cluster.Name = clusterName
			cluster.Namespace = namespace
			cluster.Spec.Instances = 1
			cluster.Spec.StorageConfiguration = apiv1.StorageConfiguration{
				Request:      "5Gi",
				Limit:        "20Gi",
				TargetBuffer: ptr.To(20),
				MaintenanceWindow: &apiv1.MaintenanceWindowConfig{
					// Set maintenance window to a time that is not now
					// robfig/cron uses 6-field format: second minute hour day-of-month month day-of-week
					Schedule: "0 0 4 31 2 *", // February 31st at 4am (never happens)
					Duration: "1h",
					Timezone: "UTC",
				},
			}

			By("creating cluster", func() {
				err := env.Client.Create(env.Ctx, cluster)
				Expect(err).ToNot(HaveOccurred())

				AssertClusterIsReady(namespace, clusterName, testTimeouts[timeouts.ClusterIsReady], env)
			})

			By("verifying growth is pending", func() {
				// Growth should be queued, not executed immediately
				Eventually(func(g Gomega) {
					cluster, err := clusterutils.Get(env.Ctx, env.Client, namespace, clusterName)
					g.Expect(err).ToNot(HaveOccurred())
					// The cluster should show next maintenance window
					if cluster.Status.StorageSizing != nil &&
						cluster.Status.StorageSizing.Data != nil &&
						cluster.Status.StorageSizing.Data.State == "PendingGrowth" {
						g.Expect(cluster.Status.StorageSizing.Data.NextMaintenanceWindow).ToNot(BeNil())
					}
				}).WithTimeout(time.Duration(testTimeouts[timeouts.AKSVolumeResize]) * time.Second).
					WithPolling(time.Duration(testTimeouts[timeouts.AKSPollingInterval]) * time.Second).Should(Succeed())
			})
		})
	})

	Context("Emergency growth", Label(tests.LabelDynamicStorage), func() {
		It("grows immediately when critical threshold is reached", func() {
			var err error
			namespace, err = env.CreateUniqueTestNamespace(env.Ctx, env.Client, namespacePrefix)
			Expect(err).ToNot(HaveOccurred())

			clusterName := "dynamic-emergency"
			cluster := &apiv1.Cluster{}
			cluster.Name = clusterName
			cluster.Namespace = namespace
			cluster.Spec.Instances = 1
			cluster.Spec.StorageConfiguration = apiv1.StorageConfiguration{
				Request:      "5Gi",
				Limit:        "20Gi",
				TargetBuffer: ptr.To(20),
				MaintenanceWindow: &apiv1.MaintenanceWindowConfig{
					// robfig/cron uses 6-field format: second minute hour day-of-month month day-of-week
					Schedule: "0 0 4 31 2 *", // Never open
					Duration: "1h",
					Timezone: "UTC",
				},
				EmergencyGrow: &apiv1.EmergencyGrowConfig{
					Enabled:             ptr.To(true),
					CriticalThreshold:   95,
					CriticalMinimumFree: "100Mi",
				},
			}

			By("creating cluster", func() {
				err := env.Client.Create(env.Ctx, cluster)
				Expect(err).ToNot(HaveOccurred())

				AssertClusterIsReady(namespace, clusterName, testTimeouts[timeouts.ClusterIsReady], env)
			})

			var primaryPod *corev1.Pod
			By("finding primary pod", func() {
				primaryPod, err = clusterutils.GetPrimary(env.Ctx, env.Client, namespace, clusterName)
				Expect(err).ToNot(HaveOccurred())
			})

			By("filling disk to emergency threshold", func() {
				// Fill disk incrementally to reach ~96% usage (past the 95% critical threshold
				// that triggers emergency growth). We use incremental filling to give the
				// storage reconciler time to detect the emergency condition and respond.
				// For emergency tests we need to push past the critical threshold (95%)
				// while still leaving room for the filesystem overhead.
				// - targetUsagePercent=96: past the 95% critical threshold
				// - maxUsagePercent=98: safety limit but allows reaching emergency level
				// - batchRows=300000: smaller batches for finer control near capacity
				finalUsage, err := fillDiskIncrementally(primaryPod, 96, 98, 300000)
				if err != nil {
					GinkgoWriter.Printf("Emergency disk fill ended with error (may be expected): %v\n", err)
				}
				GinkgoWriter.Printf("Final disk usage for emergency test: %d%%\n", finalUsage)
				// We should have reached at least 90% to be near emergency threshold
				Expect(finalUsage).To(BeNumerically(">=", 90),
					"Emergency disk fill should reach at least 90%% to approach critical threshold")
			})

			By("verifying emergency growth triggers", func() {
				// The PVC should grow despite maintenance window being closed
				Eventually(func(g Gomega) {
					cluster, err := clusterutils.Get(env.Ctx, env.Client, namespace, clusterName)
					g.Expect(err).ToNot(HaveOccurred())
					if cluster.Status.StorageSizing != nil &&
						cluster.Status.StorageSizing.Data != nil &&
						cluster.Status.StorageSizing.Data.LastAction != nil {
						g.Expect(cluster.Status.StorageSizing.Data.LastAction.Kind).To(Equal("EmergencyGrow"))
					}
				}).WithTimeout(time.Duration(testTimeouts[timeouts.AKSVolumeResize]) * time.Second).
					WithPolling(time.Duration(testTimeouts[timeouts.AKSPollingInterval]) * time.Second).Should(Succeed())
			})
		})
	})

	Context("Rate limiting", Label(tests.LabelDynamicStorage), func() {
		It("respects max actions per day budget", func() {
			var err error
			namespace, err = env.CreateUniqueTestNamespace(env.Ctx, env.Client, namespacePrefix)
			Expect(err).ToNot(HaveOccurred())

			clusterName := "dynamic-budget"
			cluster := &apiv1.Cluster{}
			cluster.Name = clusterName
			cluster.Namespace = namespace
			cluster.Spec.Instances = 1
			cluster.Spec.StorageConfiguration = apiv1.StorageConfiguration{
				Request:      "5Gi",
				Limit:        "20Gi",
				TargetBuffer: ptr.To(20),
				EmergencyGrow: &apiv1.EmergencyGrowConfig{
					Enabled:                     ptr.To(true),
					CriticalThreshold:           95,
					MaxActionsPerDay:            ptr.To(2),
					ReservedActionsForEmergency: ptr.To(1),
				},
			}

			By("creating cluster", func() {
				err := env.Client.Create(env.Ctx, cluster)
				Expect(err).ToNot(HaveOccurred())

				AssertClusterIsReady(namespace, clusterName, testTimeouts[timeouts.ClusterIsReady], env)
			})

			By("verifying budget is tracked", func() {
				Eventually(func(g Gomega) {
					cluster, err := clusterutils.Get(env.Ctx, env.Client, namespace, clusterName)
					g.Expect(err).ToNot(HaveOccurred())
					if cluster.Status.StorageSizing != nil &&
						cluster.Status.StorageSizing.Data != nil &&
						cluster.Status.StorageSizing.Data.Budget != nil {
						g.Expect(cluster.Status.StorageSizing.Data.Budget.AvailableForPlanned).To(BeNumerically(">=", 0))
						g.Expect(cluster.Status.StorageSizing.Data.Budget.AvailableForEmergency).To(BeNumerically(">=", 0))
					}
				}).WithTimeout(time.Duration(testTimeouts[timeouts.AKSVolumeResize]) * time.Second).
					WithPolling(time.Duration(testTimeouts[timeouts.AKSPollingInterval]) * time.Second).Should(Succeed())
			})
		})
	})

	// ============================================================================
	// Codex P0 E2E Scenarios - Required for Merge
	// See: docs/src/design/dynamic-storage-e2e-requirements-codex.md
	// ============================================================================

	// Test: Growth operation in progress + operator pod restart
	Context("Operator restart during growth",
		Serial, Label(tests.LabelDynamicStorage, tests.LabelDisruptive), func() {
			It("resumes growth operation after operator pod restart", func() {
				var err error
				namespace, err = env.CreateUniqueTestNamespace(env.Ctx, env.Client, namespacePrefix)
				Expect(err).ToNot(HaveOccurred())

				clusterName := "dynamic-op-restart"
				cluster := &apiv1.Cluster{}
				cluster.Name = clusterName
				cluster.Namespace = namespace
				cluster.Spec.Instances = 3
				cluster.Spec.StorageConfiguration = apiv1.StorageConfiguration{
					Request:      "5Gi",
					Limit:        "20Gi",
					TargetBuffer: ptr.To(20),
				}

				// Create test data table for data integrity verification
				tableLocator := TableLocator{
					Namespace:    namespace,
					ClusterName:  clusterName,
					DatabaseName: postgres.AppDBName,
					TableName:    "sentinel",
				}

				By("creating cluster", func() {
					err := env.Client.Create(env.Ctx, cluster)
					Expect(err).ToNot(HaveOccurred())
					AssertClusterIsReady(namespace, clusterName, testTimeouts[timeouts.ClusterIsReady], env)
				})

				By("inserting sentinel data for integrity check", func() {
					AssertCreateTestData(env, tableLocator)
				})

				var primaryPod *corev1.Pod
				By("filling disk to trigger growth", func() {
					primaryPod, err = clusterutils.GetPrimary(env.Ctx, env.Client, namespace, clusterName)
					Expect(err).ToNot(HaveOccurred())

					// Fill disk incrementally to reach ~85% usage (exceeding the 80% threshold
					// that triggers growth when targetBuffer is 20%). We use incremental filling
					// to give the storage reconciler time to detect the condition and respond.
					finalUsage, fillErr := fillDiskIncrementally(primaryPod, 85, 92, 500000)
					if fillErr != nil {
						GinkgoWriter.Printf("Disk fill ended with error (may be expected): %v\n", fillErr)
					}
					GinkgoWriter.Printf("Final disk usage after fill: %d%%\n", finalUsage)
					Expect(finalUsage).To(BeNumerically(">=", 75),
						"Disk fill should reach at least 75%% to trigger growth")
				})

				By("waiting for storage sizing status to indicate growth needed", func() {
					Eventually(func(g Gomega) {
						cluster, err := clusterutils.Get(env.Ctx, env.Client, namespace, clusterName)
						g.Expect(err).ToNot(HaveOccurred())
						g.Expect(cluster.Status.StorageSizing).ToNot(BeNil())
						g.Expect(cluster.Status.StorageSizing.Data).ToNot(BeNil())
					}).WithTimeout(time.Duration(testTimeouts[timeouts.AKSVolumeResize]) * time.Second).
						WithPolling(time.Duration(testTimeouts[timeouts.AKSPollingInterval]) * time.Second).Should(Succeed())
				})

				By("restarting operator pod during growth operation", func() {
					err := operator.ReloadDeployment(env.Ctx, env.Client, 120)
					Expect(err).ToNot(HaveOccurred())
				})

				By("verifying operator recovered and cluster converges", func() {
					AssertClusterIsReady(namespace, clusterName, testTimeouts[timeouts.ClusterIsReady], env)
				})

				By("verifying storage sizing status is consistent after restart", func() {
					Eventually(func(g Gomega) {
						cluster, err := clusterutils.Get(env.Ctx, env.Client, namespace, clusterName)
						g.Expect(err).ToNot(HaveOccurred())
						g.Expect(cluster.Status.StorageSizing).ToNot(BeNil())
					}).WithTimeout(time.Duration(testTimeouts[timeouts.AKSVolumeResize]) * time.Second).
						WithPolling(time.Duration(testTimeouts[timeouts.AKSPollingInterval]) * time.Second).Should(Succeed())
				})

				By("verifying data integrity after operator restart", func() {
					AssertDataExpectedCount(env, tableLocator, 2)
				})

				By("verifying PVCs respect request/limit bounds", func() {
					var pvcList corev1.PersistentVolumeClaimList
					err := env.Client.List(env.Ctx, &pvcList,
						ctrlclient.InNamespace(namespace),
						ctrlclient.MatchingLabels{utils.ClusterLabelName: clusterName})
					Expect(err).ToNot(HaveOccurred())
					request := resource.MustParse("5Gi")
					limit := resource.MustParse("20Gi")
					for _, pvc := range pvcList.Items {
						size := pvc.Spec.Resources.Requests[corev1.ResourceStorage]
						Expect(size.Cmp(request)).To(BeNumerically(">=", 0))
						Expect(size.Cmp(limit)).To(BeNumerically("<=", 0))
					}
				})
			})
		})

	// Test: Growth operation in progress + PostgreSQL primary pod restart
	Context("Primary pod restart during growth",
		Label(tests.LabelDynamicStorage, tests.LabelSelfHealing), func() {
			It("resumes growth operation after primary pod restart", func() {
				var err error
				namespace, err = env.CreateUniqueTestNamespace(env.Ctx, env.Client, namespacePrefix)
				Expect(err).ToNot(HaveOccurred())

				clusterName := "dynamic-pod-restart"
				cluster := &apiv1.Cluster{}
				cluster.Name = clusterName
				cluster.Namespace = namespace
				cluster.Spec.Instances = 3
				cluster.Spec.StorageConfiguration = apiv1.StorageConfiguration{
					Request:      "5Gi",
					Limit:        "20Gi",
					TargetBuffer: ptr.To(20),
				}

				tableLocator := TableLocator{
					Namespace:    namespace,
					ClusterName:  clusterName,
					DatabaseName: postgres.AppDBName,
					TableName:    "sentinel",
				}

				By("creating cluster", func() {
					err := env.Client.Create(env.Ctx, cluster)
					Expect(err).ToNot(HaveOccurred())
					AssertClusterIsReady(namespace, clusterName, testTimeouts[timeouts.ClusterIsReady], env)
				})

				By("inserting sentinel data for integrity check", func() {
					AssertCreateTestData(env, tableLocator)
				})

				var primaryPod *corev1.Pod
				By("filling disk to trigger growth", func() {
					primaryPod, err = clusterutils.GetPrimary(env.Ctx, env.Client, namespace, clusterName)
					Expect(err).ToNot(HaveOccurred())

					// Fill disk incrementally to reach ~85% usage (exceeding the 80% threshold
					// that triggers growth when targetBuffer is 20%). We use incremental filling
					// to give the storage reconciler time to detect the condition and respond.
					finalUsage, fillErr := fillDiskIncrementally(primaryPod, 85, 92, 500000)
					if fillErr != nil {
						GinkgoWriter.Printf("Disk fill ended with error (may be expected): %v\n", fillErr)
					}
					GinkgoWriter.Printf("Final disk usage after fill: %d%%\n", finalUsage)
					Expect(finalUsage).To(BeNumerically(">=", 75),
						"Disk fill should reach at least 75%% to trigger growth")
				})

				By("waiting for storage sizing status to be populated", func() {
					Eventually(func(g Gomega) {
						cluster, err := clusterutils.Get(env.Ctx, env.Client, namespace, clusterName)
						g.Expect(err).ToNot(HaveOccurred())
						g.Expect(cluster.Status.StorageSizing).ToNot(BeNil())
					}).WithTimeout(time.Duration(testTimeouts[timeouts.StorageSizingDetection]) * time.Second).
						WithPolling(time.Duration(testTimeouts[timeouts.StorageSizingPolling]) * time.Second).Should(Succeed())
				})

				By("waiting for PVC capacity to update (CSI resize completion)", func() {
					// CRITICAL: Wait for CSI driver to complete the resize before disrupting the pod.
					// Azure AKS CSI operations can take 5-10 minutes.
					waitForPVCCapacityUpdate(namespace, clusterName,
						time.Duration(testTimeouts[timeouts.AKSVolumeResize])*time.Second)
				})

				By("deleting primary pod to trigger restart", func() {
					quickDelete := &ctrlclient.DeleteOptions{
						GracePeriodSeconds: &quickDeletionPeriod,
					}
					err = podutils.Delete(env.Ctx, env.Client, namespace, primaryPod.Name, quickDelete)
					Expect(err).ToNot(HaveOccurred())
				})

				By("verifying cluster returns to Ready state", func() {
					AssertClusterIsReady(namespace, clusterName, testTimeouts[timeouts.ClusterIsReady], env)
				})

				By("verifying data integrity after primary restart", func() {
					AssertDataExpectedCount(env, tableLocator, 2)
				})

				By("verifying storage sizing continues after restart", func() {
					Eventually(func(g Gomega) {
						cluster, err := clusterutils.Get(env.Ctx, env.Client, namespace, clusterName)
						g.Expect(err).ToNot(HaveOccurred())
						g.Expect(cluster.Status.StorageSizing).ToNot(BeNil())
					}).WithTimeout(time.Duration(testTimeouts[timeouts.AKSVolumeResize]) * time.Second).
						WithPolling(time.Duration(testTimeouts[timeouts.AKSPollingInterval]) * time.Second).Should(Succeed())
				})
			})
		})

	// Test: Growth operation in progress + failover/switchover event
	Context("Failover during growth", Label(tests.LabelDynamicStorage, tests.LabelSelfHealing), func() {
		It("continues growth safely after failover", func() {
			var err error
			namespace, err = env.CreateUniqueTestNamespace(env.Ctx, env.Client, namespacePrefix)
			Expect(err).ToNot(HaveOccurred())

			clusterName := "dynamic-failover"
			cluster := &apiv1.Cluster{}
			cluster.Name = clusterName
			cluster.Namespace = namespace
			cluster.Spec.Instances = 3
			cluster.Spec.StorageConfiguration = apiv1.StorageConfiguration{
				Request:      "5Gi",
				Limit:        "20Gi",
				TargetBuffer: ptr.To(20),
			}

			tableLocator := TableLocator{
				Namespace:    namespace,
				ClusterName:  clusterName,
				DatabaseName: postgres.AppDBName,
				TableName:    "sentinel",
			}

			By("creating cluster", func() {
				err := env.Client.Create(env.Ctx, cluster)
				Expect(err).ToNot(HaveOccurred())
				AssertClusterIsReady(namespace, clusterName, testTimeouts[timeouts.ClusterIsReady], env)
			})

			By("inserting sentinel data for integrity check", func() {
				AssertCreateTestData(env, tableLocator)
			})

			var originalPrimary string
			By("recording original primary and filling disk", func() {
				primaryPod, err := clusterutils.GetPrimary(env.Ctx, env.Client, namespace, clusterName)
				Expect(err).ToNot(HaveOccurred())
				originalPrimary = primaryPod.Name

				// Fill disk incrementally to reach ~85% usage (exceeding the 80% threshold
				// that triggers growth when targetBuffer is 20%). We use incremental filling
				// to give the storage reconciler time to detect the condition and respond.
				finalUsage, fillErr := fillDiskIncrementally(primaryPod, 85, 92, 500000)
				if fillErr != nil {
					GinkgoWriter.Printf("Disk fill ended with error (may be expected): %v\n", fillErr)
				}
				GinkgoWriter.Printf("Final disk usage after fill: %d%%\n", finalUsage)
				Expect(finalUsage).To(BeNumerically(">=", 75),
					"Disk fill should reach at least 75%% to trigger growth")
			})

			By("waiting for storage sizing to be active", func() {
				Eventually(func(g Gomega) {
					cluster, err := clusterutils.Get(env.Ctx, env.Client, namespace, clusterName)
					g.Expect(err).ToNot(HaveOccurred())
					g.Expect(cluster.Status.StorageSizing).ToNot(BeNil())
				}).WithTimeout(time.Duration(testTimeouts[timeouts.StorageSizingDetection]) * time.Second).
					WithPolling(time.Duration(testTimeouts[timeouts.StorageSizingPolling]) * time.Second).Should(Succeed())
			})

			By("waiting for PVC capacity to update (CSI resize completion)", func() {
				// CRITICAL: Wait for CSI driver to complete the resize before triggering switchover.
				// Azure AKS CSI operations can take 5-10 minutes. If we switchover while the
				// CSI driver is still resizing, the operation may fail or the cluster may not stabilize.
				waitForPVCCapacityUpdate(namespace, clusterName,
					time.Duration(testTimeouts[timeouts.AKSVolumeResize])*time.Second)
			})

			By("triggering switchover", func() {
				// Instead of using AssertSwitchover (which has hardcoded 120s timeout),
				// we manually trigger the switchover and use configurable timeouts.
				// This is necessary for Azure AKS which may be slower.

				// Get current primary
				cluster, err := clusterutils.Get(env.Ctx, env.Client, namespace, clusterName)
				Expect(err).ToNot(HaveOccurred())
				currentPrimary := cluster.Status.CurrentPrimary

				// Get pod list to find a target
				podList, err := clusterutils.ListPods(env.Ctx, env.Client, namespace, clusterName)
				Expect(err).ToNot(HaveOccurred())
				Expect(len(podList.Items)).To(BeNumerically(">=", 2))

				// Find a replica to promote
				var targetPrimary string
				for _, pod := range podList.Items {
					if pod.Name != currentPrimary {
						targetPrimary = pod.Name
						break
					}
				}
				Expect(targetPrimary).ToNot(BeEmpty())

				// Trigger switchover by setting TargetPrimary in status
				originCluster := cluster.DeepCopy()
				cluster.Status.TargetPrimary = targetPrimary
				err = env.Client.Status().Patch(env.Ctx, cluster, ctrlclient.MergeFrom(originCluster))
				Expect(err).ToNot(HaveOccurred())

				// Wait for switchover to complete with longer timeout
				// After a storage resize on AKS, pods need additional time to stabilize
				// before they can successfully complete a switchover. Use the AKS-specific
				// timeout which accounts for CSI operations.
				Eventually(func(g Gomega) {
					cluster, err := clusterutils.Get(env.Ctx, env.Client, namespace, clusterName)
					g.Expect(err).ToNot(HaveOccurred())
					g.Expect(cluster.Status.CurrentPrimary).To(Equal(targetPrimary))
				}).WithTimeout(time.Duration(testTimeouts[timeouts.AKSVolumeResize]) * time.Second).
					WithPolling(time.Duration(testTimeouts[timeouts.AKSPollingInterval]) * time.Second).Should(Succeed())

				// Wait for cluster to be ready (using the configured timeout)
				AssertClusterIsReady(namespace, clusterName, testTimeouts[timeouts.ClusterIsReady], env)
			})

			By("verifying correct primary election", func() {
				Eventually(func(g Gomega) {
					cluster, err := clusterutils.Get(env.Ctx, env.Client, namespace, clusterName)
					g.Expect(err).ToNot(HaveOccurred())
					g.Expect(cluster.Status.CurrentPrimary).ToNot(Equal(originalPrimary))
				}).WithTimeout(time.Duration(testTimeouts[timeouts.AKSVolumeResize]) * time.Second).
					WithPolling(time.Duration(testTimeouts[timeouts.AKSPollingInterval]) * time.Second).Should(Succeed())
			})

			By("verifying data integrity after switchover", func() {
				AssertDataExpectedCount(env, tableLocator, 2)
			})

			By("verifying no size divergence across instances", func() {
				var pvcList corev1.PersistentVolumeClaimList
				err := env.Client.List(env.Ctx, &pvcList,
					ctrlclient.InNamespace(namespace),
					ctrlclient.MatchingLabels{utils.ClusterLabelName: clusterName})
				Expect(err).ToNot(HaveOccurred())

				// All PVCs should be within request/limit bounds
				request := resource.MustParse("5Gi")
				limit := resource.MustParse("20Gi")
				for _, pvc := range pvcList.Items {
					size := pvc.Spec.Resources.Requests[corev1.ResourceStorage]
					Expect(size.Cmp(request)).To(BeNumerically(">=", 0))
					Expect(size.Cmp(limit)).To(BeNumerically("<=", 0))
				}
			})
		})
	})

	// Test: Growth operation in progress + user spec mutation
	Context("Spec mutation during growth", Label(tests.LabelDynamicStorage), func() {
		It("re-evaluates plan deterministically when spec changes", func() {
			var err error
			namespace, err = env.CreateUniqueTestNamespace(env.Ctx, env.Client, namespacePrefix)
			Expect(err).ToNot(HaveOccurred())

			clusterName := "dynamic-spec-change"
			cluster := &apiv1.Cluster{}
			cluster.Name = clusterName
			cluster.Namespace = namespace
			cluster.Spec.Instances = 2
			cluster.Spec.StorageConfiguration = apiv1.StorageConfiguration{
				Request:      "5Gi",
				Limit:        "15Gi",
				TargetBuffer: ptr.To(20),
			}

			tableLocator := TableLocator{
				Namespace:    namespace,
				ClusterName:  clusterName,
				DatabaseName: postgres.AppDBName,
				TableName:    "sentinel",
			}

			By("creating cluster", func() {
				err := env.Client.Create(env.Ctx, cluster)
				Expect(err).ToNot(HaveOccurred())
				AssertClusterIsReady(namespace, clusterName, testTimeouts[timeouts.ClusterIsReady], env)
			})

			By("inserting sentinel data", func() {
				AssertCreateTestData(env, tableLocator)
			})

			By("filling disk to trigger growth", func() {
				primaryPod, err := clusterutils.GetPrimary(env.Ctx, env.Client, namespace, clusterName)
				Expect(err).ToNot(HaveOccurred())

				// Fill disk incrementally to reach ~85% usage (exceeding the 80% threshold
				// that triggers growth when targetBuffer is 20%). We use incremental filling
				// to give the storage reconciler time to detect the condition and respond.
				finalUsage, fillErr := fillDiskIncrementally(primaryPod, 85, 92, 500000)
				if fillErr != nil {
					GinkgoWriter.Printf("Disk fill ended with error (may be expected): %v\n", fillErr)
				}
				GinkgoWriter.Printf("Final disk usage after fill: %d%%\n", finalUsage)
				Expect(finalUsage).To(BeNumerically(">=", 75),
					"Disk fill should reach at least 75%% to trigger growth")
			})

			By("waiting for storage sizing to be active", func() {
				Eventually(func(g Gomega) {
					cluster, err := clusterutils.Get(env.Ctx, env.Client, namespace, clusterName)
					g.Expect(err).ToNot(HaveOccurred())
					g.Expect(cluster.Status.StorageSizing).ToNot(BeNil())
				}).WithTimeout(time.Duration(testTimeouts[timeouts.AKSVolumeResize]) * time.Second).
					WithPolling(time.Duration(testTimeouts[timeouts.AKSPollingInterval]) * time.Second).Should(Succeed())
			})

			By("mutating spec: increasing limit", func() {
				cluster, err := clusterutils.Get(env.Ctx, env.Client, namespace, clusterName)
				Expect(err).ToNot(HaveOccurred())

				originCluster := cluster.DeepCopy()
				cluster.Spec.StorageConfiguration.Limit = "25Gi"
				err = env.Client.Patch(env.Ctx, cluster, ctrlclient.MergeFrom(originCluster))
				Expect(err).ToNot(HaveOccurred())
			})

			By("verifying reconciler accepts new spec", func() {
				Eventually(func(g Gomega) {
					cluster, err := clusterutils.Get(env.Ctx, env.Client, namespace, clusterName)
					g.Expect(err).ToNot(HaveOccurred())
					g.Expect(cluster.Spec.StorageConfiguration.Limit).To(Equal("25Gi"))
				}).WithTimeout(time.Duration(testTimeouts[timeouts.AKSVolumeResize]) * time.Second).
					WithPolling(time.Duration(testTimeouts[timeouts.AKSPollingInterval]) * time.Second).Should(Succeed())
			})

			By("mutating spec: adjusting targetBuffer", func() {
				cluster, err := clusterutils.Get(env.Ctx, env.Client, namespace, clusterName)
				Expect(err).ToNot(HaveOccurred())

				originCluster := cluster.DeepCopy()
				cluster.Spec.StorageConfiguration.TargetBuffer = ptr.To(30)
				err = env.Client.Patch(env.Ctx, cluster, ctrlclient.MergeFrom(originCluster))
				Expect(err).ToNot(HaveOccurred())
			})

			By("verifying cluster remains in healthy state", func() {
				AssertClusterIsReady(namespace, clusterName, testTimeouts[timeouts.ClusterIsReady], env)
			})

			By("verifying data integrity", func() {
				AssertDataExpectedCount(env, tableLocator, 2)
			})

			By("verifying PVCs respect new limit", func() {
				var pvcList corev1.PersistentVolumeClaimList
				err := env.Client.List(env.Ctx, &pvcList,
					ctrlclient.InNamespace(namespace),
					ctrlclient.MatchingLabels{utils.ClusterLabelName: clusterName})
				Expect(err).ToNot(HaveOccurred())
				newLimit := resource.MustParse("25Gi")
				for _, pvc := range pvcList.Items {
					size := pvc.Spec.Resources.Requests[corev1.ResourceStorage]
					Expect(size.Cmp(newLimit)).To(BeNumerically("<=", 0))
				}
			})
		})
	})

	// Test: Growth operation in progress + node cordon/drain affecting active instance
	Context("Node drain during growth",
		Serial, Label(tests.LabelDynamicStorage, tests.LabelDisruptive, tests.LabelMaintenance), func() {
			It("recovers growth operation after node drain", func() {
				var err error
				namespace, err = env.CreateUniqueTestNamespace(env.Ctx, env.Client, namespacePrefix)
				Expect(err).ToNot(HaveOccurred())

				clusterName := "dynamic-node-drain"
				cluster := &apiv1.Cluster{}
				cluster.Name = clusterName
				cluster.Namespace = namespace
				cluster.Spec.Instances = 3
				cluster.Spec.StorageConfiguration = apiv1.StorageConfiguration{
					Request:      "5Gi",
					Limit:        "20Gi",
					TargetBuffer: ptr.To(20),
				}

				tableLocator := TableLocator{
					Namespace:    namespace,
					ClusterName:  clusterName,
					DatabaseName: postgres.AppDBName,
					TableName:    "sentinel",
				}

				By("creating cluster", func() {
					err := env.Client.Create(env.Ctx, cluster)
					Expect(err).ToNot(HaveOccurred())
					AssertClusterIsReady(namespace, clusterName, testTimeouts[timeouts.ClusterIsReady], env)
				})

				By("inserting sentinel data", func() {
					AssertCreateTestData(env, tableLocator)
				})

				By("filling disk to trigger growth", func() {
					primaryPod, err := clusterutils.GetPrimary(env.Ctx, env.Client, namespace, clusterName)
					Expect(err).ToNot(HaveOccurred())

					// Fill disk incrementally to reach ~85% usage (exceeding the 80% threshold
					// that triggers growth when targetBuffer is 20%). We use incremental filling
					// to give the storage reconciler time to detect the condition and respond.
					finalUsage, fillErr := fillDiskIncrementally(primaryPod, 85, 92, 500000)
					if fillErr != nil {
						GinkgoWriter.Printf("Disk fill ended with error (may be expected): %v\n", fillErr)
					}
					GinkgoWriter.Printf("Final disk usage after fill: %d%%\n", finalUsage)
					Expect(finalUsage).To(BeNumerically(">=", 75),
						"Disk fill should reach at least 75%% to trigger growth")
				})

				By("waiting for storage sizing to be active", func() {
					Eventually(func(g Gomega) {
						cluster, err := clusterutils.Get(env.Ctx, env.Client, namespace, clusterName)
						g.Expect(err).ToNot(HaveOccurred())
						g.Expect(cluster.Status.StorageSizing).ToNot(BeNil())
					}).WithTimeout(time.Duration(testTimeouts[timeouts.AKSVolumeResize]) * time.Second).
						WithPolling(time.Duration(testTimeouts[timeouts.AKSPollingInterval]) * time.Second).Should(Succeed())
				})

				By("draining node containing primary", func() {
					// DrainPrimary cordons and drains the node containing the primary pod
					podsOnPrimaryNode := nodes.DrainPrimary(
						env.Ctx, env.Client,
						namespace, clusterName,
						testTimeouts[timeouts.DrainNode],
					)
					Expect(podsOnPrimaryNode).ToNot(BeEmpty())
				})

				By("verifying cluster converges after drain", func() {
					// Uncordon nodes to allow pods to be rescheduled
					err := nodes.UncordonAll(env.Ctx, env.Client)
					Expect(err).ToNot(HaveOccurred())

					AssertClusterIsReady(namespace, clusterName, testTimeouts[timeouts.ClusterIsReady], env)
				})

				By("verifying storage sizing continues after drain", func() {
					Eventually(func(g Gomega) {
						cluster, err := clusterutils.Get(env.Ctx, env.Client, namespace, clusterName)
						g.Expect(err).ToNot(HaveOccurred())
						g.Expect(cluster.Status.StorageSizing).ToNot(BeNil())
					}).WithTimeout(time.Duration(testTimeouts[timeouts.AKSVolumeResize]) * time.Second).
						WithPolling(time.Duration(testTimeouts[timeouts.AKSPollingInterval]) * time.Second).Should(Succeed())
				})

				By("verifying data integrity after drain", func() {
					AssertDataExpectedCount(env, tableLocator, 2)
				})

				By("verifying PVCs respect bounds after drain", func() {
					var pvcList corev1.PersistentVolumeClaimList
					err := env.Client.List(env.Ctx, &pvcList,
						ctrlclient.InNamespace(namespace),
						ctrlclient.MatchingLabels{utils.ClusterLabelName: clusterName})
					Expect(err).ToNot(HaveOccurred())

					request := resource.MustParse("5Gi")
					limit := resource.MustParse("20Gi")
					for _, pvc := range pvcList.Items {
						size := pvc.Spec.Resources.Requests[corev1.ResourceStorage]
						Expect(size.Cmp(request)).To(BeNumerically(">=", 0))
						Expect(size.Cmp(limit)).To(BeNumerically("<=", 0))
					}
				})
			})
		})

	// Test: Growth operation in progress + backup creation
	Context("Backup during growth", Label(tests.LabelDynamicStorage), func() {
		It("backup succeeds or fails clearly without deadlocking storage reconciliation", func() {
			var err error
			namespace, err = env.CreateUniqueTestNamespace(env.Ctx, env.Client, namespacePrefix)
			Expect(err).ToNot(HaveOccurred())

			clusterName := "dynamic-backup"
			cluster := &apiv1.Cluster{}
			cluster.Name = clusterName
			cluster.Namespace = namespace
			cluster.Spec.Instances = 2
			cluster.Spec.StorageConfiguration = apiv1.StorageConfiguration{
				Request:      "5Gi",
				Limit:        "20Gi",
				TargetBuffer: ptr.To(20),
			}

			tableLocator := TableLocator{
				Namespace:    namespace,
				ClusterName:  clusterName,
				DatabaseName: postgres.AppDBName,
				TableName:    "sentinel",
			}

			By("creating cluster", func() {
				err := env.Client.Create(env.Ctx, cluster)
				Expect(err).ToNot(HaveOccurred())
				AssertClusterIsReady(namespace, clusterName, testTimeouts[timeouts.ClusterIsReady], env)
			})

			By("inserting sentinel data", func() {
				AssertCreateTestData(env, tableLocator)
			})

			By("filling disk to trigger growth", func() {
				primaryPod, err := clusterutils.GetPrimary(env.Ctx, env.Client, namespace, clusterName)
				Expect(err).ToNot(HaveOccurred())

				// Fill disk incrementally to reach ~85% usage (exceeding the 80% threshold
				// that triggers growth when targetBuffer is 20%). We use incremental filling
				// to give the storage reconciler time to detect the condition and respond.
				finalUsage, fillErr := fillDiskIncrementally(primaryPod, 85, 92, 500000)
				if fillErr != nil {
					GinkgoWriter.Printf("Disk fill ended with error (may be expected): %v\n", fillErr)
				}
				GinkgoWriter.Printf("Final disk usage after fill: %d%%\n", finalUsage)
				Expect(finalUsage).To(BeNumerically(">=", 75),
					"Disk fill should reach at least 75%% to trigger growth")
			})

			By("waiting for storage sizing to be active", func() {
				Eventually(func(g Gomega) {
					cluster, err := clusterutils.Get(env.Ctx, env.Client, namespace, clusterName)
					g.Expect(err).ToNot(HaveOccurred())
					g.Expect(cluster.Status.StorageSizing).ToNot(BeNil())
				}).WithTimeout(time.Duration(testTimeouts[timeouts.AKSVolumeResize]) * time.Second).
					WithPolling(time.Duration(testTimeouts[timeouts.AKSPollingInterval]) * time.Second).Should(Succeed())
			})

			// Note: Full backup test requires object storage (MinIO/Azure).
			// This test verifies that the storage sizing controller doesn't deadlock
			// when backup-related operations are happening concurrently.

			By("verifying cluster remains healthy and storage sizing continues", func() {
				AssertClusterIsReady(namespace, clusterName, testTimeouts[timeouts.ClusterIsReady], env)

				Eventually(func(g Gomega) {
					cluster, err := clusterutils.Get(env.Ctx, env.Client, namespace, clusterName)
					g.Expect(err).ToNot(HaveOccurred())
					g.Expect(cluster.Status.StorageSizing).ToNot(BeNil())
				}).WithTimeout(time.Duration(testTimeouts[timeouts.AKSVolumeResize]) * time.Second).
					WithPolling(time.Duration(testTimeouts[timeouts.AKSPollingInterval]) * time.Second).Should(Succeed())
			})

			By("verifying data integrity", func() {
				AssertDataExpectedCount(env, tableLocator, 2)
			})
		})
	})

	// Test: New replica scale-up during/after prior dynamic resize
	Context("Replica scale-up after resize", Label(tests.LabelDynamicStorage), func() {
		It("creates new replica at effective operational size", func() {
			var err error
			namespace, err = env.CreateUniqueTestNamespace(env.Ctx, env.Client, namespacePrefix)
			Expect(err).ToNot(HaveOccurred())

			clusterName := "dynamic-replica-size"
			cluster := &apiv1.Cluster{}
			cluster.Name = clusterName
			cluster.Namespace = namespace
			cluster.Spec.Instances = 1
			cluster.Spec.StorageConfiguration = apiv1.StorageConfiguration{
				Request:      "5Gi",
				Limit:        "20Gi",
				TargetBuffer: ptr.To(20),
			}

			tableLocator := TableLocator{
				Namespace:    namespace,
				ClusterName:  clusterName,
				DatabaseName: postgres.AppDBName,
				TableName:    "sentinel",
			}

			By("creating cluster with 1 instance", func() {
				err := env.Client.Create(env.Ctx, cluster)
				Expect(err).ToNot(HaveOccurred())
				AssertClusterIsReady(namespace, clusterName, testTimeouts[timeouts.ClusterIsReady], env)
			})

			By("inserting sentinel data", func() {
				AssertCreateTestData(env, tableLocator)
			})

			By("recording original PVC size", func() {
				var pvcList corev1.PersistentVolumeClaimList
				err := env.Client.List(env.Ctx, &pvcList,
					ctrlclient.InNamespace(namespace),
					ctrlclient.MatchingLabels{utils.ClusterLabelName: clusterName})
				Expect(err).ToNot(HaveOccurred())
				Expect(pvcList.Items).To(HaveLen(1))
				originalPVCSize := pvcList.Items[0].Spec.Resources.Requests[corev1.ResourceStorage]
				GinkgoWriter.Printf("Original PVC size: %s\n", originalPVCSize.String())
			})

			By("filling disk to trigger growth", func() {
				primaryPod, err := clusterutils.GetPrimary(env.Ctx, env.Client, namespace, clusterName)
				Expect(err).ToNot(HaveOccurred())

				// Fill disk incrementally to reach ~85% usage (exceeding the 80% threshold
				// that triggers growth when targetBuffer is 20%). We use incremental filling
				// to give the storage reconciler time to detect the condition and respond.
				finalUsage, fillErr := fillDiskIncrementally(primaryPod, 85, 92, 500000)
				if fillErr != nil {
					GinkgoWriter.Printf("Disk fill ended with error (may be expected): %v\n", fillErr)
				}
				GinkgoWriter.Printf("Final disk usage after fill: %d%%\n", finalUsage)
				Expect(finalUsage).To(BeNumerically(">=", 75),
					"Disk fill should reach at least 75%% to trigger growth")
			})

			By("waiting for storage sizing status to be populated", func() {
				Eventually(func(g Gomega) {
					cluster, err := clusterutils.Get(env.Ctx, env.Client, namespace, clusterName)
					g.Expect(err).ToNot(HaveOccurred())
					g.Expect(cluster.Status.StorageSizing).ToNot(BeNil())
					g.Expect(cluster.Status.StorageSizing.Data).ToNot(BeNil())
				}).WithTimeout(time.Duration(testTimeouts[timeouts.AKSVolumeResize]) * time.Second).
					WithPolling(time.Duration(testTimeouts[timeouts.AKSPollingInterval]) * time.Second).Should(Succeed())
			})

			// The effective size may or may not be set depending on whether growth was triggered.
			// What matters is that new replicas match the current PVC size of existing instances.
			var currentPVCSize resource.Quantity
			By("recording current PVC size", func() {
				var pvcList corev1.PersistentVolumeClaimList
				err := env.Client.List(env.Ctx, &pvcList,
					ctrlclient.InNamespace(namespace),
					ctrlclient.MatchingLabels{utils.ClusterLabelName: clusterName})
				Expect(err).ToNot(HaveOccurred())
				Expect(pvcList.Items).To(HaveLen(1))
				currentPVCSize = pvcList.Items[0].Spec.Resources.Requests[corev1.ResourceStorage]
				GinkgoWriter.Printf("Current PVC size: %s\n", currentPVCSize.String())
			})

			// Get effective size if available, otherwise use current PVC size
			var effectiveSize string
			By("recording effective size from status or using current PVC size", func() {
				cluster, err := clusterutils.Get(env.Ctx, env.Client, namespace, clusterName)
				Expect(err).ToNot(HaveOccurred())
				if cluster.Status.StorageSizing.Data.EffectiveSize != "" {
					effectiveSize = cluster.Status.StorageSizing.Data.EffectiveSize
				} else {
					// If effective size is not set, use the current PVC size
					effectiveSize = currentPVCSize.String()
				}
				GinkgoWriter.Printf("Effective size to use: %s\n", effectiveSize)
			})

			By("scaling to 2 instances", func() {
				cluster, err := clusterutils.Get(env.Ctx, env.Client, namespace, clusterName)
				Expect(err).ToNot(HaveOccurred())

				originCluster := cluster.DeepCopy()
				cluster.Spec.Instances = 2
				err = env.Client.Patch(env.Ctx, cluster, ctrlclient.MergeFrom(originCluster))
				Expect(err).ToNot(HaveOccurred())

				AssertClusterIsReady(namespace, clusterName, testTimeouts[timeouts.ClusterIsReady], env)
			})

			By("verifying new replica PVC is created at effective size", func() {
				Eventually(func(g Gomega) {
					var pvcList corev1.PersistentVolumeClaimList
					err := env.Client.List(env.Ctx, &pvcList,
						ctrlclient.InNamespace(namespace),
						ctrlclient.MatchingLabels{utils.ClusterLabelName: clusterName})
					g.Expect(err).ToNot(HaveOccurred())
					g.Expect(pvcList.Items).To(HaveLen(2))

					// New replica should be at effective size, not the stale request size
					effectiveSizeQty := resource.MustParse(effectiveSize)
					for _, pvc := range pvcList.Items {
						size := pvc.Spec.Resources.Requests[corev1.ResourceStorage]
						// All PVCs should be at or near effective size (may be slightly larger due to PV rounding)
						g.Expect(size.Cmp(effectiveSizeQty)).To(BeNumerically(">=", 0),
							fmt.Sprintf("PVC %s size %s should be >= effective size %s",
								pvc.Name, size.String(), effectiveSize))
					}
				}).WithTimeout(time.Duration(testTimeouts[timeouts.AKSVolumeResize]) * time.Second).
					WithPolling(time.Duration(testTimeouts[timeouts.AKSPollingInterval]) * time.Second).Should(Succeed())
			})

			By("verifying data integrity on new replica", func() {
				AssertDataExpectedCount(env, tableLocator, 2)
			})
		})
	})

	// Test: Daily action-budget or rate-limit boundaries
	Context("Rate limiting", Label(tests.LabelDynamicStorage), func() {
		It("respects planned/emergency action budget and exposes exhaustion in status", func() {
			var err error
			namespace, err = env.CreateUniqueTestNamespace(env.Ctx, env.Client, namespacePrefix)
			Expect(err).ToNot(HaveOccurred())

			clusterName := "dynamic-budget"
			cluster := &apiv1.Cluster{}
			cluster.Name = clusterName
			cluster.Namespace = namespace
			cluster.Spec.Instances = 1
			cluster.Spec.StorageConfiguration = apiv1.StorageConfiguration{
				Request:      "5Gi",
				Limit:        "30Gi",
				TargetBuffer: ptr.To(20),
				EmergencyGrow: &apiv1.EmergencyGrowConfig{
					Enabled:                     ptr.To(true),
					CriticalThreshold:           95,
					MaxActionsPerDay:            ptr.To(3),
					ReservedActionsForEmergency: ptr.To(1),
				},
			}

			By("creating cluster", func() {
				err := env.Client.Create(env.Ctx, cluster)
				Expect(err).ToNot(HaveOccurred())
				AssertClusterIsReady(namespace, clusterName, testTimeouts[timeouts.ClusterIsReady], env)
			})

			By("verifying budget is initialized correctly", func() {
				Eventually(func(g Gomega) {
					cluster, err := clusterutils.Get(env.Ctx, env.Client, namespace, clusterName)
					g.Expect(err).ToNot(HaveOccurred())
					if cluster.Status.StorageSizing != nil &&
						cluster.Status.StorageSizing.Data != nil &&
						cluster.Status.StorageSizing.Data.Budget != nil {
						budget := cluster.Status.StorageSizing.Data.Budget
						// With maxActionsPerDay=3 and reservedForEmergency=1:
						// availableForPlanned should be 2, availableForEmergency should be 1
						g.Expect(budget.AvailableForPlanned).To(BeNumerically(">=", 0))
						g.Expect(budget.AvailableForEmergency).To(BeNumerically(">=", 0))
						// Budget reset time should be set
						g.Expect(budget.BudgetResetsAt.IsZero()).To(BeFalse())
					}
				}).WithTimeout(time.Duration(testTimeouts[timeouts.AKSVolumeResize]) * time.Second).
					WithPolling(time.Duration(testTimeouts[timeouts.AKSPollingInterval]) * time.Second).Should(Succeed())
			})

			By("verifying budget status is exposed in cluster status", func() {
				Eventually(func(g Gomega) {
					cluster, err := clusterutils.Get(env.Ctx, env.Client, namespace, clusterName)
					g.Expect(err).ToNot(HaveOccurred())
					g.Expect(cluster.Status.StorageSizing).ToNot(BeNil())
					g.Expect(cluster.Status.StorageSizing.Data).ToNot(BeNil())
					g.Expect(cluster.Status.StorageSizing.Data.Budget).ToNot(BeNil())
				}).WithTimeout(time.Duration(testTimeouts[timeouts.AKSVolumeResize]) * time.Second).
					WithPolling(time.Duration(testTimeouts[timeouts.AKSPollingInterval]) * time.Second).Should(Succeed())
			})
		})
	})

	// ============================================================================
	// Topology Matrix Tests (T1, T2, T3)
	// Per requirements, P0 scenarios must run in each topology
	// ============================================================================

	Context("Topology T1: Single instance (instances=1)", Label(tests.LabelDynamicStorage), func() {
		It("handles dynamic sizing with no replicas", func() {
			var err error
			namespace, err = env.CreateUniqueTestNamespace(env.Ctx, env.Client, namespacePrefix)
			Expect(err).ToNot(HaveOccurred())

			clusterName := "dynamic-t1"
			cluster := &apiv1.Cluster{}
			cluster.Name = clusterName
			cluster.Namespace = namespace
			cluster.Spec.Instances = 1
			cluster.Spec.StorageConfiguration = apiv1.StorageConfiguration{
				Request:      "5Gi",
				Limit:        "20Gi",
				TargetBuffer: ptr.To(20),
			}

			tableLocator := TableLocator{
				Namespace:    namespace,
				ClusterName:  clusterName,
				DatabaseName: postgres.AppDBName,
				TableName:    "sentinel",
			}

			By("creating single-instance cluster", func() {
				err := env.Client.Create(env.Ctx, cluster)
				Expect(err).ToNot(HaveOccurred())
				AssertClusterIsReady(namespace, clusterName, testTimeouts[timeouts.ClusterIsReady], env)
			})

			By("inserting sentinel data", func() {
				AssertCreateTestData(env, tableLocator)
			})

			By("filling disk to trigger growth", func() {
				primaryPod, err := clusterutils.GetPrimary(env.Ctx, env.Client, namespace, clusterName)
				Expect(err).ToNot(HaveOccurred())

				// Fill disk incrementally to reach ~85% usage (exceeding the 80% threshold
				// that triggers growth when targetBuffer is 20%). We use incremental filling
				// to give the storage reconciler time to detect the condition and respond.
				finalUsage, fillErr := fillDiskIncrementally(primaryPod, 85, 92, 500000)
				if fillErr != nil {
					GinkgoWriter.Printf("Disk fill ended with error (may be expected): %v\n", fillErr)
				}
				GinkgoWriter.Printf("Final disk usage after fill: %d%%\n", finalUsage)
				Expect(finalUsage).To(BeNumerically(">=", 75),
					"Disk fill should reach at least 75%% to trigger growth")
			})

			By("verifying storage sizing works with T1 topology", func() {
				Eventually(func(g Gomega) {
					cluster, err := clusterutils.Get(env.Ctx, env.Client, namespace, clusterName)
					g.Expect(err).ToNot(HaveOccurred())
					g.Expect(cluster.Status.StorageSizing).ToNot(BeNil())
				}).WithTimeout(time.Duration(testTimeouts[timeouts.StorageSizingDetection]) * time.Second).
					WithPolling(time.Duration(testTimeouts[timeouts.StorageSizingPolling]) * time.Second).Should(Succeed())
			})

			By("waiting for PVC capacity to update (CSI resize completion)", func() {
				// For T1 topology (single instance), we must wait for the CSI resize to complete
				// before verifying data integrity, as the instance may be affected by the resize.
				waitForPVCCapacityUpdate(namespace, clusterName,
					time.Duration(testTimeouts[timeouts.AKSVolumeResize])*time.Second)
			})

			By("verifying data integrity", func() {
				AssertDataExpectedCount(env, tableLocator, 2)
			})
		})
	})

	Context("Topology T2: Two instances (instances=2)", Label(tests.LabelDynamicStorage), func() {
		It("handles dynamic sizing with single replica", func() {
			var err error
			namespace, err = env.CreateUniqueTestNamespace(env.Ctx, env.Client, namespacePrefix)
			Expect(err).ToNot(HaveOccurred())

			clusterName := "dynamic-t2"
			cluster := &apiv1.Cluster{}
			cluster.Name = clusterName
			cluster.Namespace = namespace
			cluster.Spec.Instances = 2
			cluster.Spec.StorageConfiguration = apiv1.StorageConfiguration{
				Request:      "5Gi",
				Limit:        "20Gi",
				TargetBuffer: ptr.To(20),
			}

			tableLocator := TableLocator{
				Namespace:    namespace,
				ClusterName:  clusterName,
				DatabaseName: postgres.AppDBName,
				TableName:    "sentinel",
			}

			By("creating two-instance cluster", func() {
				err := env.Client.Create(env.Ctx, cluster)
				Expect(err).ToNot(HaveOccurred())
				AssertClusterIsReady(namespace, clusterName, testTimeouts[timeouts.ClusterIsReady], env)
			})

			By("inserting sentinel data", func() {
				AssertCreateTestData(env, tableLocator)
			})

			By("filling disk to trigger growth", func() {
				primaryPod, err := clusterutils.GetPrimary(env.Ctx, env.Client, namespace, clusterName)
				Expect(err).ToNot(HaveOccurred())

				// Fill disk incrementally to reach ~85% usage (exceeding the 80% threshold
				// that triggers growth when targetBuffer is 20%). We use incremental filling
				// to give the storage reconciler time to detect the condition and respond.
				finalUsage, fillErr := fillDiskIncrementally(primaryPod, 85, 92, 500000)
				if fillErr != nil {
					GinkgoWriter.Printf("Disk fill ended with error (may be expected): %v\n", fillErr)
				}
				GinkgoWriter.Printf("Final disk usage after fill: %d%%\n", finalUsage)
				Expect(finalUsage).To(BeNumerically(">=", 75),
					"Disk fill should reach at least 75%% to trigger growth")
			})

			By("verifying storage sizing works with T2 topology", func() {
				Eventually(func(g Gomega) {
					cluster, err := clusterutils.Get(env.Ctx, env.Client, namespace, clusterName)
					g.Expect(err).ToNot(HaveOccurred())
					g.Expect(cluster.Status.StorageSizing).ToNot(BeNil())
				}).WithTimeout(time.Duration(testTimeouts[timeouts.StorageSizingDetection]) * time.Second).
					WithPolling(time.Duration(testTimeouts[timeouts.StorageSizingPolling]) * time.Second).Should(Succeed())
			})

			By("waiting for PVC capacity to update (CSI resize completion)", func() {
				// CRITICAL: Wait for CSI driver to complete the resize before switchover.
				// Azure AKS CSI operations can take 5-10 minutes.
				waitForPVCCapacityUpdate(namespace, clusterName,
					time.Duration(testTimeouts[timeouts.AKSVolumeResize])*time.Second)
			})

			By("verifying promotion/replica replacement ordering is safe", func() {
				// Trigger a switchover to verify ordering
				AssertSwitchover(namespace, clusterName, env)
			})

			By("verifying data integrity after switchover", func() {
				AssertDataExpectedCount(env, tableLocator, 2)
			})

			By("verifying all PVCs are consistent", func() {
				var pvcList corev1.PersistentVolumeClaimList
				err := env.Client.List(env.Ctx, &pvcList,
					ctrlclient.InNamespace(namespace),
					ctrlclient.MatchingLabels{utils.ClusterLabelName: clusterName})
				Expect(err).ToNot(HaveOccurred())
				Expect(pvcList.Items).To(HaveLen(2))

				request := resource.MustParse("5Gi")
				limit := resource.MustParse("20Gi")
				for _, pvc := range pvcList.Items {
					size := pvc.Spec.Resources.Requests[corev1.ResourceStorage]
					Expect(size.Cmp(request)).To(BeNumerically(">=", 0))
					Expect(size.Cmp(limit)).To(BeNumerically("<=", 0))
				}
			})
		})
	})

	Context("Topology T3: Multiple replicas (instances>=3)", Label(tests.LabelDynamicStorage), func() {
		It("handles dynamic sizing with multiple replicas without unnecessary churn", func() {
			var err error
			namespace, err = env.CreateUniqueTestNamespace(env.Ctx, env.Client, namespacePrefix)
			Expect(err).ToNot(HaveOccurred())

			clusterName := "dynamic-t3"
			cluster := &apiv1.Cluster{}
			cluster.Name = clusterName
			cluster.Namespace = namespace
			cluster.Spec.Instances = 3
			cluster.Spec.StorageConfiguration = apiv1.StorageConfiguration{
				Request:      "5Gi",
				Limit:        "20Gi",
				TargetBuffer: ptr.To(20),
			}

			tableLocator := TableLocator{
				Namespace:    namespace,
				ClusterName:  clusterName,
				DatabaseName: postgres.AppDBName,
				TableName:    "sentinel",
			}

			By("creating three-instance cluster", func() {
				err := env.Client.Create(env.Ctx, cluster)
				Expect(err).ToNot(HaveOccurred())
				AssertClusterIsReady(namespace, clusterName, testTimeouts[timeouts.ClusterIsReady], env)
			})

			By("inserting sentinel data", func() {
				AssertCreateTestData(env, tableLocator)
			})

			// Record initial pod UIDs to check for unnecessary churn
			initialPodUIDs := make(map[string]types.UID)
			By("recording initial pod UIDs", func() {
				podList, err := clusterutils.ListPods(env.Ctx, env.Client, namespace, clusterName)
				Expect(err).ToNot(HaveOccurred())
				for _, pod := range podList.Items {
					initialPodUIDs[pod.Name] = pod.UID
				}
			})

			By("filling disk to trigger growth", func() {
				primaryPod, err := clusterutils.GetPrimary(env.Ctx, env.Client, namespace, clusterName)
				Expect(err).ToNot(HaveOccurred())

				// Fill disk incrementally to reach ~85% usage (exceeding the 80% threshold
				// that triggers growth when targetBuffer is 20%). We use incremental filling
				// to give the storage reconciler time to detect the condition and respond.
				finalUsage, fillErr := fillDiskIncrementally(primaryPod, 85, 92, 500000)
				if fillErr != nil {
					GinkgoWriter.Printf("Disk fill ended with error (may be expected): %v\n", fillErr)
				}
				GinkgoWriter.Printf("Final disk usage after fill: %d%%\n", finalUsage)
				Expect(finalUsage).To(BeNumerically(">=", 75),
					"Disk fill should reach at least 75%% to trigger growth")
			})

			By("verifying storage sizing works with T3 topology", func() {
				Eventually(func(g Gomega) {
					cluster, err := clusterutils.Get(env.Ctx, env.Client, namespace, clusterName)
					g.Expect(err).ToNot(HaveOccurred())
					g.Expect(cluster.Status.StorageSizing).ToNot(BeNil())
				}).WithTimeout(time.Duration(testTimeouts[timeouts.StorageSizingDetection]) * time.Second).
					WithPolling(time.Duration(testTimeouts[timeouts.StorageSizingPolling]) * time.Second).Should(Succeed())
			})

			By("waiting for PVC capacity to update (CSI resize completion)", func() {
				// CRITICAL: Wait for CSI driver to complete the resize before checking PVC consistency.
				// Azure AKS CSI operations can take 5-10 minutes.
				waitForPVCCapacityUpdate(namespace, clusterName,
					time.Duration(testTimeouts[timeouts.AKSVolumeResize])*time.Second)
			})

			By("verifying data integrity", func() {
				AssertDataExpectedCount(env, tableLocator, 2)
			})

			By("verifying no unnecessary multi-node churn", func() {
				// Wait for storage sizing reconciliation to stabilize by watching for
				// a consistent state, rather than using a fixed sleep.
				// We verify that pods remain stable (no unnecessary restarts) by checking
				// that pod UIDs haven't changed after the storage sizing status stabilizes.
				Eventually(func(g Gomega) {
					cluster, err := clusterutils.Get(env.Ctx, env.Client, namespace, clusterName)
					g.Expect(err).ToNot(HaveOccurred())
					// Wait for storage sizing status to reach a stable state
					g.Expect(cluster.Status.StorageSizing).ToNot(BeNil())
					g.Expect(cluster.Status.StorageSizing.Data).ToNot(BeNil())
					// State should be one of the stable states (Balanced, PendingGrowth, etc.)
					// not actively Resizing
					state := cluster.Status.StorageSizing.Data.State
					g.Expect(state).ToNot(Equal("Resizing"),
						"Waiting for storage sizing to complete resizing")
				}).WithTimeout(time.Duration(testTimeouts[timeouts.AKSVolumeResize]) * time.Second).
					WithPolling(time.Duration(testTimeouts[timeouts.AKSPollingInterval]) * time.Second).Should(Succeed())

				podList, err := clusterutils.ListPods(env.Ctx, env.Client, namespace, clusterName)
				Expect(err).ToNot(HaveOccurred())

				// Count how many pods were replaced (UID changed)
				replacedPods := 0
				for _, pod := range podList.Items {
					if originalUID, exists := initialPodUIDs[pod.Name]; exists {
						if pod.UID != originalUID {
							replacedPods++
							GinkgoWriter.Printf("Pod %s was replaced (old UID: %s, new UID: %s)\n",
								pod.Name, originalUID, pod.UID)
						}
					}
				}
				// Storage resize should not cause unnecessary pod replacements
				// (online resize should keep pods running)
				Expect(replacedPods).To(BeNumerically("<=", 1),
					"Expected at most 1 pod replacement for storage resize, got %d", replacedPods)
			})

			By("verifying quorum-safe behavior (3 instances running)", func() {
				Eventually(func(g Gomega) {
					cluster, err := clusterutils.Get(env.Ctx, env.Client, namespace, clusterName)
					g.Expect(err).ToNot(HaveOccurred())
					g.Expect(cluster.Status.ReadyInstances).To(Equal(3))
				}).WithTimeout(time.Duration(testTimeouts[timeouts.AKSVolumeResize]) * time.Second).
					WithPolling(time.Duration(testTimeouts[timeouts.AKSPollingInterval]) * time.Second).Should(Succeed())
			})

			By("verifying all PVCs are consistent", func() {
				var pvcList corev1.PersistentVolumeClaimList
				err := env.Client.List(env.Ctx, &pvcList,
					ctrlclient.InNamespace(namespace),
					ctrlclient.MatchingLabels{utils.ClusterLabelName: clusterName})
				Expect(err).ToNot(HaveOccurred())
				Expect(pvcList.Items).To(HaveLen(3))

				request := resource.MustParse("5Gi")
				limit := resource.MustParse("20Gi")
				for _, pvc := range pvcList.Items {
					size := pvc.Spec.Resources.Requests[corev1.ResourceStorage]
					Expect(size.Cmp(request)).To(BeNumerically(">=", 0))
					Expect(size.Cmp(limit)).To(BeNumerically("<=", 0))
				}
			})
		})
	})
})
